{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Function Assignment Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Explain the role of activation functions in neural networks. Compare and contrast linear and nonlinear activation functions. Why are nonlinear activation functions preferred in hidden layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### --> Activation functions play a crucial role in neural networks by determining the output of each neuron, effectively influencing the network's ability to learn and make complex decisions. Specifically, activation functions introduce non-linearity to the model, enabling it to approximate complex patterns and relationships between the input and output data.\n",
    "* **Linear activation Function :-**\n",
    "Linear activations might be useful in the output layer of certain types of neural networks, especially when predicting continuous values in regression tasks. However, they are generally avoided in hidden layers.Linear functions cannot model complex, non-linear relationships. This limits the model’s power and restricts it from solving complex tasks.\n",
    "* **Non Linear activation Function :-** \n",
    "Nonlinear activation functions introduce non-linearity into the model, which allows the neural network to approximate complex relationships. Common nonlinear activations include ReLU (Rectified Linear Unit), Sigmoid, Tanh, and Leaky ReLU.Some nonlinear functions (e.g., Sigmoid, Tanh) can suffer from vanishing gradients, especially in deep networks.Nonlinearity sometimes adds computational complexity, but this is generally manageable with modern hardware and optimizations.\n",
    "\n",
    "##### Nonlinear activation functions are indispensable for deep learning, enabling neural networks to generalize across complex data patterns. They allow neural networks to make use of depth, with each layer learning unique, hierarchical features essential for tasks like image recognition, language processing, and beyond. In contrast, linear functions lack the power to model non-linear relationships, significantly limiting their utility in deep architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Describe the Sigmoid activation function. What are its characteristics, and in what type of layers is it commonly used? Explain the Rectified Linear Unit (ReLU) activation function. Discuss its advantages and potential challenges.What is the purpose of the Tanh activation function? How does it differ from the Sigmoid activation function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ---> **Sigmoid activation Function**\n",
    "The sigmoid activation function is a mathematical function commonly used in neural networks to transform input values into a range between 0 and 1.It is defined by the formula: $$f(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "where \n",
    "x is the input to the neuron.\n",
    "\n",
    "* **Characteristics of Sigmoid:-**\n",
    "* Output Range: 0 to 1. This bounded range is useful for probabilities or binary classifications where the result can represent a probability-like interpretation.\n",
    "* Smooth Gradient: The function has a smooth gradient, making it continuous and differentiable.\n",
    "Non-linearity: Although non-linear, the Sigmoid activation can lead to issues with gradient values, especially in deep networks.\n",
    "* Vanishing Gradient Problem: In cases where the inputs are very large or very small, the gradient of the sigmoid function becomes extremely small (close to 0), which can make learning slower in deep networks as backpropagation has little gradient to propagate.\n",
    " ##### **Common Usage:-**\n",
    "The sigmoid function was initially used across all layers but is now generally limited to output layers of binary classification models, where it outputs a value representing the probability of a class (0 or 1). However, it is less commonly used in hidden layers of deep networks due to its vanishing gradient issue.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Rectified Linear Unit(ReLU) Activation Function:-**\n",
    "The ReLU activation function is defined as: $$f(x) = \\max(0, x)$$\n",
    "This means it outputs zero if the input is negative, and outputs the input value itself if it’s positive.\n",
    "* **Characteristics of ReLU:-**\n",
    "* Output Range: 0 to infinity for positive inputs, and exactly 0 for negative inputs.\n",
    "* Non-linearity: ReLU introduces non-linearity, allowing the network to learn complex functions.\n",
    "* Computationally Efficient: ReLU is simple and fast to compute, making it suitable for large networks.\n",
    "* Sparsity: Many neurons in a layer will output zero when using ReLU, which helps create sparse activations, saving memory and making computation more efficient.\n",
    "* **Advantages:-**\n",
    "* Relu is widely used in hidden layers of deep neural networks beacuse of its efficiency and effectiveness in learning representations.\n",
    "* Solves the Vanishing gradient problem and its gradient is 1 for positive inputs, so it doesn't suffer from vanishing gradients.\n",
    "* Since negative inputs are zeroed out, ReLU creates sparse representations, meaning fewer neurons activate simultaneously. This sparsity often leads to faster computation and can act as a form of regularization.\n",
    "* **Challenges:-**\n",
    "* ReLU can sometimes lead to neurons permanently dying, where they output zero for all inputs and stop contributing to learning. This occurs if the weights are updated in such a way that the neuron’s input is always negative. Once a neuron \"dies,\" it is unlikely to recover, especially if it continues to receive negative inputs.\n",
    "* The gradient of ReLU is undefined at zero, but in practice, this is rarely an issue as deep learning frameworks manage it effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Tanh Activation Function :-**\n",
    "The Tanh activation function (hyperbolic tangent) is given by: $$f(x) = \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "It maps inputs to a range between -1 and 1.\n",
    "* **Characteristics of Tanh:-**\n",
    "* Output Range: -1 to 1, which centers the data around zero.\n",
    "* Gradient Range: Similar to sigmoid, the gradients of tanh saturate (become small) at large positive and negative inputs, which can lead to the vanishing gradient problem.\n",
    "* Symmetry: Tanh is zero-centered, which often leads to faster convergence compared to sigmoid. It balances positive and negative values around zero, which can help in certain types of optimization.\n",
    "* **Comparison with Sigmoid:-**\n",
    "* Range: Sigmoid ranges between 0 and 1, while tanh ranges between -1 and 1. This makes tanh zero-centered, which is often beneficial for training convergence.\n",
    "* Gradient Saturation: Both functions can suffer from vanishing gradients for very large or small inputs, but tanh has a broader middle range with higher gradients, making it generally perform better than sigmoid in hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Discuss the significance of activation functions in the hidden layers of a neural network-\n",
    "Activation functions play a fundamental role in the hidden layers of a neural network, as they are key to enabling the network to learn complex patterns, hierarchies, and non-linear relationships in the data.Significance:-\n",
    "* Activation functions introduce non-linearity to the network, allowing it to model non-linear relationships between inputs and outputs.\n",
    "* Activation functions in hidden layers enable each layer to learn new representations and abstractions of the input data. This process is crucial for building a hierarchy of features — from simple, low-level features in initial layers to complex, high-level representations in deeper layers.\n",
    "* Non-linear activation functions allow neural networks to stack many hidden layers, each capturing new levels of abstraction. This enables the model to develop a sophisticated understanding of the input data, increasing its representational power.\n",
    "* Activation functions in hidden layers are what make deep networks useful for tasks that involve highly complex mappings, such as translation, object detection, and speech recognition.\n",
    "* Activation functions help control how much of the input signal flows to the next layer.\n",
    "* Activation functions like Tanh and Sigmoid can saturate for very large or small inputs, meaning their gradients can approach zero in these regions, which slows down learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. - Explain the choice of activation functions for different types of problems (e.g., classification, regression) in the output layer-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ---> \n",
    "| Problem Type                    | Activation Function   | Reason                                           |\n",
    "|---------------------------------|-----------------------|--------------------------------------------------|\n",
    "| Binary Classification           | Sigmoid              | Outputs probability between 0 and 1              |\n",
    "| Multi-Class Classification       | Softmax             | Provides probability distribution across classes |\n",
    "| Multi-Label Classification       | Sigmoid (per output)| Independent probabilities for each label         |\n",
    "| Regression (Single Output)       | Linear              | Unbounded continuous output                      |\n",
    "| Regression (Multiple Output)     | Linear (per output) | Unbounded continuous output for each target      |\n",
    "| Regression (Bounded Range)       | Sigmoid or Tanh     | Constrains output to specified range             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.  Experiment with different activation functions (e.g., ReLU, Sigmoid, Tanh) in a simple neural network architecture. Compare their effects on convergence and performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with ReLU Activation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Downloads File\\pandas practise file\\DL_classes\\dl_env\\Lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.6529 - loss: 1.3263\n",
      "Epoch 2/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8816 - loss: 0.4465\n",
      "Epoch 3/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9035 - loss: 0.3569\n",
      "Epoch 4/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9132 - loss: 0.3164\n",
      "Epoch 5/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9148 - loss: 0.2977\n",
      "ReLU Activation - Test Accuracy: 92.41%\n",
      "\n",
      "Training with Sigmoid Activation:\n",
      "Epoch 1/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.4788 - loss: 2.0200\n",
      "Epoch 2/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7989 - loss: 1.1823\n",
      "Epoch 3/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8409 - loss: 0.8008\n",
      "Epoch 4/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8602 - loss: 0.6364\n",
      "Epoch 5/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8695 - loss: 0.5500\n",
      "Sigmoid Activation - Test Accuracy: 88.20%\n",
      "\n",
      "Training with Tanh Activation:\n",
      "Epoch 1/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.6991 - loss: 1.1723\n",
      "Epoch 2/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8798 - loss: 0.4530\n",
      "Epoch 3/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8946 - loss: 0.3784\n",
      "Epoch 4/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9065 - loss: 0.3364\n",
      "Epoch 5/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9101 - loss: 0.3222\n",
      "Tanh Activation - Test Accuracy: 91.62%\n",
      "\n",
      "\n",
      "Summary of Results:\n",
      "ReLU Activation - Test Accuracy: 92.41%\n",
      "Sigmoid Activation - Test Accuracy: 88.20%\n",
      "Tanh Activation - Test Accuracy: 91.62%\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Set up the Experiment\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize data\n",
    "x_train = x_train.reshape(-1, 28 * 28).astype(\"float32\")\n",
    "x_test = x_test.reshape(-1, 28 * 28).astype(\"float32\")\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 784\n",
    "hidden_size = 128\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Step 2: Define a Function to Create the Model\n",
    "def create_model(activation_fn):\n",
    "    model = models.Sequential([\n",
    "        layers.InputLayer(input_shape=(input_size,)),\n",
    "        layers.Dense(hidden_size, activation=activation_fn),\n",
    "        layers.Dense(num_classes, activation='softmax')  # Softmax for multi-class classification\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Step 3: Define a Function to Train and Evaluate the Model\n",
    "def train_and_evaluate(activation_fn, activation_name):\n",
    "    print(f\"Training with {activation_name} Activation:\")\n",
    "    \n",
    "    model = create_model(activation_fn)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(x_train, y_train, epochs=num_epochs, batch_size=batch_size, verbose=1)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(f\"{activation_name} Activation - Test Accuracy: {test_accuracy * 100:.2f}%\\n\")\n",
    "    \n",
    "    return test_accuracy * 100, history.history['loss']\n",
    "\n",
    "# Step 4: Run Experiments with Different Activation Functions\n",
    "# Experiment with ReLU, Sigmoid, and Tanh activation functions\n",
    "activations = {\n",
    "    \"ReLU\": 'relu',\n",
    "    \"Sigmoid\": 'sigmoid',\n",
    "    \"Tanh\": 'tanh'\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, activation in activations.items():\n",
    "    accuracy, loss_history = train_and_evaluate(activation, name)\n",
    "    results[name] = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"loss_history\": loss_history\n",
    "    }\n",
    "\n",
    "# Summary of Results\n",
    "print(\"\\nSummary of Results:\")\n",
    "for name, result in results.items():\n",
    "    print(f\"{name} Activation - Test Accuracy: {result['accuracy']:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:-**\n",
    "Relu Activation Function gives highest test accuracy and 2nd is Tanh activation Function and then sigmoid activation Function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
